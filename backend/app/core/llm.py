"""
LLM Client - OpenAIå°è£…
æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
"""
from typing import List, Dict, Optional, AsyncGenerator
import openai
from openai import OpenAI, AsyncOpenAI
import os


class LLMClient:
    """LLMå®¢æˆ·ç«¯å°è£…ï¼ˆæ”¯æŒOpenAI/DeepSeekå¤šæ¨¡å¼åˆ‡æ¢+è‡ªåŠ¨é™çº§ï¼‰"""

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4"):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            api_key: API Keyï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å¼é€‰æ‹©ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å¼é€‰æ‹©ï¼‰
        """
        # ğŸ”§ è¯»å–LLMæ¨¡å¼ï¼ˆauto | openai | deepseekï¼‰
        self.llm_mode = os.getenv("LLM_MODE", "auto").lower()

        # æ ¹æ®æ¨¡å¼é€‰æ‹©é…ç½®
        if self.llm_mode == "auto":
            # è‡ªåŠ¨é™çº§æ¨¡å¼ï¼šåŒæ—¶åˆå§‹åŒ–ä¸¤ä¸ªå®¢æˆ·ç«¯
            print(f"ğŸŒ LLMæ¨¡å¼: è‡ªåŠ¨é™çº§ï¼ˆOpenAIä¼˜å…ˆ â†’ DeepSeekå…œåº•ï¼‰")

            # OpenAIé…ç½®
            self.openai_api_key = os.getenv("OPENAI_API_KEY")
            self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4")

            # DeepSeeké…ç½®
            self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
            self.deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

            # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
            if self.openai_api_key:
                self.openai_async_client = AsyncOpenAI(api_key=self.openai_api_key)
                print(f"  âœ… OpenAI ({self.openai_model}) å·²å°±ç»ª")
            else:
                self.openai_async_client = None
                print(f"  âš ï¸  OpenAI API Keyæœªè®¾ç½®")

            # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
            if self.deepseek_api_key:
                self.deepseek_async_client = AsyncOpenAI(
                    api_key=self.deepseek_api_key,
                    base_url="https://api.deepseek.com/v1"
                )
                print(f"  âœ… DeepSeek ({self.deepseek_model}) å·²å°±ç»ªï¼ˆå…œåº•ï¼‰")
            else:
                self.deepseek_async_client = None
                print(f"  âš ï¸  DeepSeek API Keyæœªè®¾ç½®")

            # ä¸ºäº†å…¼å®¹æ—§ä»£ç ï¼Œè®¾ç½®é»˜è®¤å®¢æˆ·ç«¯
            self.async_client = self.openai_async_client
            self.api_key = self.openai_api_key
            self.model = self.openai_model

        elif self.llm_mode == "deepseek":
            # DeepSeekæ¨¡å¼
            self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
            self.model = model or os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
            self.base_url = "https://api.deepseek.com/v1"
            print(f"ğŸŒ LLMæ¨¡å¼: DeepSeek ({self.model}) - å›½å†…ç›´è¿")

            if self.api_key:
                self.async_client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
            else:
                self.async_client = None
                print(f"âš ï¸  è­¦å‘Š: DeepSeek API Keyæœªè®¾ç½®")

        else:  # openaiï¼ˆé»˜è®¤ï¼‰
            # OpenAIæ¨¡å¼
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            self.model = model or os.getenv("OPENAI_MODEL", "gpt-4")
            self.base_url = None  # OpenAIé»˜è®¤åœ°å€
            print(f"ğŸŒ LLMæ¨¡å¼: OpenAI ({self.model})")

            if self.api_key:
                self.async_client = AsyncOpenAI(api_key=self.api_key)
            else:
                self.async_client = None
                print(f"âš ï¸  è­¦å‘Š: OpenAI API Keyæœªè®¾ç½®")

        # åˆå§‹åŒ–åŒæ­¥å®¢æˆ·ç«¯ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…æ”¯æŒä¸»æ¨¡å¼ï¼‰
        if hasattr(self, 'api_key') and self.api_key:
            client_kwargs = {"api_key": self.api_key}
            if hasattr(self, 'base_url') and self.base_url:
                client_kwargs["base_url"] = self.base_url
            self.client = OpenAI(**client_kwargs)
        else:
            self.client = None

    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Dict:
        """
        è°ƒç”¨OpenAI Chat Completion

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            å“åº”ç»“æœ
        """
        if not self.client:
            return {
                "error": "OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–",
                "content": None
            }

        try:
            response = self.client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )

            if stream:
                return {"stream": response}
            else:
                return {
                    "content": response.choices[0].message.content,
                    "finish_reason": response.choices[0].finish_reason,
                    "usage": response.usage.model_dump() if response.usage else None
                }

        except Exception as e:
            return {
                "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e)}",
                "content": None
            }

    async def async_chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
    ) -> Dict:
        """
        å¼‚æ­¥è°ƒç”¨OpenAI Chat Completion

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            å“åº”ç»“æœ
        """
        if not self.async_client:
            return {
                "error": "OpenAIå¼‚æ­¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–",
                "content": None
            }

        try:
            response = await self.async_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )

            return {
                "content": response.choices[0].message.content,
                "finish_reason": response.choices[0].finish_reason,
                "usage": response.usage.model_dump() if response.usage else None
            }

        except Exception as e:
            return {
                "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e)}",
                "content": None
            }

    async def async_chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        tools: Optional[List[Dict]] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨ï¼ˆæ”¯æŒ Function Calling + è‡ªåŠ¨é™çº§ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            tools: OpenAI tools å®šä¹‰ï¼ˆå¯é€‰ï¼‰

        Yields:
            æµå¼æ•°æ®åŒ…ï¼š
            - {"type": "content", "content": "..."}  # æ–‡æœ¬å†…å®¹
            - {"type": "tool_call", "tool_call": {...}}  # å·¥å…·è°ƒç”¨
            - {"type": "done", "finish_reason": "..."}  # å®Œæˆ
        """
        # è‡ªåŠ¨é™çº§æ¨¡å¼
        if self.llm_mode == "auto":
            # ä¼˜å…ˆå°è¯• OpenAI
            if self.openai_async_client:
                try:
                    print(f"ğŸš€ å°è¯•ä½¿ç”¨ OpenAI ({self.openai_model})...")
                    async for chunk in self._stream_with_client(
                        self.openai_async_client,
                        self.openai_model,
                        messages,
                        temperature,
                        tools
                    ):
                        yield chunk
                    print(f"âœ… OpenAI è°ƒç”¨æˆåŠŸ")
                    return  # æˆåŠŸï¼Œç›´æ¥è¿”å›

                except Exception as e:
                    error_msg = str(e)
                    print(f"âš ï¸  OpenAI è°ƒç”¨å¤±è´¥: {error_msg}")

                    # åˆ¤æ–­æ˜¯å¦æ˜¯è¶…æ—¶æˆ–ç½‘ç»œé”™è¯¯
                    if "timeout" in error_msg.lower() or "timed out" in error_msg.lower() or "connection" in error_msg.lower():
                        # è‡ªåŠ¨åˆ‡æ¢åˆ° DeepSeek
                        if self.deepseek_async_client:
                            print(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ° DeepSeek ({self.deepseek_model})...")
                            try:
                                async for chunk in self._stream_with_client(
                                    self.deepseek_async_client,
                                    self.deepseek_model,
                                    messages,
                                    temperature,
                                    tools
                                ):
                                    yield chunk
                                print(f"âœ… DeepSeek å…œåº•æˆåŠŸ")
                                return
                            except Exception as e2:
                                yield {"type": "error", "error": f"OpenAIå’ŒDeepSeekå‡å¤±è´¥: {error_msg}, {str(e2)}"}
                                return
                        else:
                            yield {"type": "error", "error": f"OpenAIå¤±è´¥ä¸”DeepSeekæœªé…ç½®: {error_msg}"}
                            return
                    else:
                        # éç½‘ç»œé”™è¯¯ï¼Œç›´æ¥è¿”å›é”™è¯¯
                        yield {"type": "error", "error": error_msg}
                        return

            # å¦‚æœ OpenAI æœªé…ç½®ï¼Œç›´æ¥ä½¿ç”¨ DeepSeek
            elif self.deepseek_async_client:
                print(f"ğŸŒ OpenAIæœªé…ç½®ï¼Œä½¿ç”¨ DeepSeek ({self.deepseek_model})...")
                async for chunk in self._stream_with_client(
                    self.deepseek_async_client,
                    self.deepseek_model,
                    messages,
                    temperature,
                    tools
                ):
                    yield chunk
                return

            else:
                yield {"type": "error", "error": "OpenAIå’ŒDeepSeekå‡æœªé…ç½®"}
                return

        # å•ä¸€æ¨¡å¼ï¼ˆopenai æˆ– deepseekï¼‰
        else:
            if not self.async_client:
                yield {"type": "error", "error": "å¼‚æ­¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}
                return

            async for chunk in self._stream_with_client(
                self.async_client,
                model or self.model,
                messages,
                temperature,
                tools
            ):
                yield chunk

    async def _stream_with_client(
        self,
        client: AsyncOpenAI,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float,
        tools: Optional[List[Dict]] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        ä½¿ç”¨æŒ‡å®šå®¢æˆ·ç«¯è¿›è¡Œæµå¼è°ƒç”¨ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        """
        try:
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "stream": True
            }

            # å¦‚æœæä¾›äº† toolsï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"

            stream = await client.chat.completions.create(**kwargs)

            tool_calls_buffer = []  # ç´¯ç§¯å·¥å…·è°ƒç”¨

            async for chunk in stream:
                delta = chunk.choices[0].delta
                finish_reason = chunk.choices[0].finish_reason

                # æ–‡æœ¬å†…å®¹
                if delta.content:
                    yield {"type": "content", "content": delta.content}

                # å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        index = tool_call.index

                        # ç¡®ä¿bufferæœ‰è¶³å¤Ÿçš„ç©ºé—´
                        while len(tool_calls_buffer) <= index:
                            tool_calls_buffer.append({
                                "id": "",
                                "type": "function",
                                "function": {"name": "", "arguments": ""}
                            })

                        # ç´¯ç§¯å·¥å…·è°ƒç”¨æ•°æ®
                        if tool_call.id:
                            tool_calls_buffer[index]["id"] = tool_call.id
                        if tool_call.function:
                            if tool_call.function.name:
                                tool_calls_buffer[index]["function"]["name"] = tool_call.function.name
                            if tool_call.function.arguments:
                                tool_calls_buffer[index]["function"]["arguments"] += tool_call.function.arguments

                # å®Œæˆ
                if finish_reason:
                    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå‘é€å®Œæ•´çš„å·¥å…·è°ƒç”¨æ•°æ®
                    if tool_calls_buffer:
                        yield {
                            "type": "tool_calls",
                            "tool_calls": tool_calls_buffer,
                            "finish_reason": finish_reason
                        }

                    yield {
                        "type": "done",
                        "finish_reason": finish_reason
                    }

        except Exception as e:
            raise e  # å‘ä¸ŠæŠ›å‡ºï¼Œç”±è°ƒç”¨æ–¹å¤„ç†

    def build_messages_with_memory(
        self,
        system_prompt: str,
        user_input: str,
        memory_context: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> List[Dict[str, str]]:
        """
        æ„å»ºå¸¦è®°å¿†çš„æ¶ˆæ¯åˆ—è¡¨

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_input: ç”¨æˆ·è¾“å…¥
            memory_context: è®°å¿†ä¸Šä¸‹æ–‡
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        messages = [
            {
                "role": "system",
                "content": f"{system_prompt}\n\n{memory_context}"
            }
        ]

        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            messages.extend(conversation_history)

        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        messages.append({
            "role": "user",
            "content": user_input
        })

        return messages


# å•ä¾‹æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
_llm_client_instance = None


def get_llm_client() -> LLMClient:
    """è·å–LLMå®¢æˆ·ç«¯å•ä¾‹"""
    global _llm_client_instance
    if _llm_client_instance is None:
        _llm_client_instance = LLMClient()
    return _llm_client_instance

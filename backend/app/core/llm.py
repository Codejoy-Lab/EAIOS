"""
LLM Client - OpenAIå°è£…
æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
"""
from typing import List, Dict, Optional, AsyncGenerator
import openai
from openai import OpenAI, AsyncOpenAI
import os


class LLMClient:
    """LLMå®¢æˆ·ç«¯å°è£…ï¼ˆæ”¯æŒOpenAI/DeepSeekå¤šæ¨¡å¼åˆ‡æ¢ï¼‰"""

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4"):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            api_key: API Keyï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å¼é€‰æ‹©ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å¼é€‰æ‹©ï¼‰
        """
        # ğŸ”§ è¯»å–LLMæ¨¡å¼ï¼ˆopenai | deepseekï¼‰
        self.llm_mode = os.getenv("LLM_MODE", "openai").lower()

        # æ ¹æ®æ¨¡å¼é€‰æ‹©é…ç½®
        if self.llm_mode == "deepseek":
            self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
            self.model = model or os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
            self.base_url = "https://api.deepseek.com/v1"
            print(f"ğŸŒ LLMæ¨¡å¼: DeepSeek ({self.model}) - å›½å†…ç›´è¿")
        else:  # openaiï¼ˆé»˜è®¤ï¼‰
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            self.model = model or os.getenv("OPENAI_MODEL", "gpt-4")
            self.base_url = None  # OpenAIé»˜è®¤åœ°å€
            print(f"ğŸŒ LLMæ¨¡å¼: OpenAI ({self.model})")

        if not self.api_key:
            print(f"âš ï¸  è­¦å‘Š: {self.llm_mode.upper()} API Keyæœªè®¾ç½®")

        # åˆå§‹åŒ–åŒæ­¥å®¢æˆ·ç«¯
        if self.api_key:
            client_kwargs = {"api_key": self.api_key}
            if self.base_url:
                client_kwargs["base_url"] = self.base_url
            self.client = OpenAI(**client_kwargs)
        else:
            self.client = None

        # åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
        if self.api_key:
            async_kwargs = {"api_key": self.api_key}
            if self.base_url:
                async_kwargs["base_url"] = self.base_url
            self.async_client = AsyncOpenAI(**async_kwargs)
        else:
            self.async_client = None

    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Dict:
        """
        è°ƒç”¨OpenAI Chat Completion

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            å“åº”ç»“æœ
        """
        if not self.client:
            return {
                "error": "OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–",
                "content": None
            }

        try:
            response = self.client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )

            if stream:
                return {"stream": response}
            else:
                return {
                    "content": response.choices[0].message.content,
                    "finish_reason": response.choices[0].finish_reason,
                    "usage": response.usage.model_dump() if response.usage else None
                }

        except Exception as e:
            return {
                "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e)}",
                "content": None
            }

    async def async_chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
    ) -> Dict:
        """
        å¼‚æ­¥è°ƒç”¨OpenAI Chat Completion

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            å“åº”ç»“æœ
        """
        if not self.async_client:
            return {
                "error": "OpenAIå¼‚æ­¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–",
                "content": None
            }

        try:
            response = await self.async_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )

            return {
                "content": response.choices[0].message.content,
                "finish_reason": response.choices[0].finish_reason,
                "usage": response.usage.model_dump() if response.usage else None
            }

        except Exception as e:
            return {
                "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e)}",
                "content": None
            }

    async def async_chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        tools: Optional[List[Dict]] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨ï¼ˆæ”¯æŒ Function Callingï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦
            tools: OpenAI tools å®šä¹‰ï¼ˆå¯é€‰ï¼‰

        Yields:
            æµå¼æ•°æ®åŒ…ï¼š
            - {"type": "content", "content": "..."}  # æ–‡æœ¬å†…å®¹
            - {"type": "tool_call", "tool_call": {...}}  # å·¥å…·è°ƒç”¨
            - {"type": "done", "finish_reason": "..."}  # å®Œæˆ
        """
        if not self.async_client:
            yield {"type": "error", "error": "OpenAIå¼‚æ­¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}
            return

        try:
            kwargs = {
                "model": model or self.model,
                "messages": messages,
                "temperature": temperature,
                "stream": True
            }

            # å¦‚æœæä¾›äº† toolsï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"

            stream = await self.async_client.chat.completions.create(**kwargs)

            tool_calls_buffer = []  # ç´¯ç§¯å·¥å…·è°ƒç”¨

            async for chunk in stream:
                delta = chunk.choices[0].delta
                finish_reason = chunk.choices[0].finish_reason

                # æ–‡æœ¬å†…å®¹
                if delta.content:
                    yield {"type": "content", "content": delta.content}

                # å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        index = tool_call.index

                        # ç¡®ä¿bufferæœ‰è¶³å¤Ÿçš„ç©ºé—´
                        while len(tool_calls_buffer) <= index:
                            tool_calls_buffer.append({
                                "id": "",
                                "type": "function",
                                "function": {"name": "", "arguments": ""}
                            })

                        # ç´¯ç§¯å·¥å…·è°ƒç”¨æ•°æ®
                        if tool_call.id:
                            tool_calls_buffer[index]["id"] = tool_call.id
                        if tool_call.function:
                            if tool_call.function.name:
                                tool_calls_buffer[index]["function"]["name"] = tool_call.function.name
                            if tool_call.function.arguments:
                                tool_calls_buffer[index]["function"]["arguments"] += tool_call.function.arguments

                # å®Œæˆ
                if finish_reason:
                    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå‘é€å®Œæ•´çš„å·¥å…·è°ƒç”¨æ•°æ®
                    if tool_calls_buffer:
                        yield {
                            "type": "tool_calls",
                            "tool_calls": tool_calls_buffer,
                            "finish_reason": finish_reason
                        }

                    yield {
                        "type": "done",
                        "finish_reason": finish_reason
                    }

        except Exception as e:
            yield {"type": "error", "error": str(e)}

    def build_messages_with_memory(
        self,
        system_prompt: str,
        user_input: str,
        memory_context: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> List[Dict[str, str]]:
        """
        æ„å»ºå¸¦è®°å¿†çš„æ¶ˆæ¯åˆ—è¡¨

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_input: ç”¨æˆ·è¾“å…¥
            memory_context: è®°å¿†ä¸Šä¸‹æ–‡
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        messages = [
            {
                "role": "system",
                "content": f"{system_prompt}\n\n{memory_context}"
            }
        ]

        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            messages.extend(conversation_history)

        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        messages.append({
            "role": "user",
            "content": user_input
        })

        return messages


# å•ä¾‹æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
_llm_client_instance = None


def get_llm_client() -> LLMClient:
    """è·å–LLMå®¢æˆ·ç«¯å•ä¾‹"""
    global _llm_client_instance
    if _llm_client_instance is None:
        _llm_client_instance = LLMClient()
    return _llm_client_instance
